
# all outdated


# from
# https://google.github.io/seq2seq/nmt/

# what do the special tokens like UNK mean?
# from https://github.com/nicolas-ivanov/tf_seq2seq_chatbot/issues/15
# GO - the same as <start> on the picture below - the first token which is fed to the 
# decoder along with the though vector in order to start generating tokens of the answer
# EOS - "end of sentence" - the same as <end> on the picture below - as soon as decoder 
# generates this token we consider the answer to be complete (you can't use usual 
# punctuation marks for this purpose cause their meaning can be different)
# UNK - "unknown token" - is used to replace the rare words that did not fit in your 
# vocabulary. So your sentence My name is guotong1988 will be translated into My name is 
# _unk_.
# PAD - padding


# set tensorflow path
~/tensorflow_1_env.sh 
export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64
export CUDADIR=/usr/local/cuda-10.0


# set environment variables to contain paths for our data
# ${HOME} is already set

# ../seq2seq/nmt_data/birdsOfEmpire/train/sources.txt

#export VOCAB_SOURCE=${HOME}/nmt_data/toy_reverse/train/vocab.sources.txt
#export VOCAB_TARGET=${HOME}/nmt_data/toy_reverse/train/vocab.targets.txt
#export TRAIN_SOURCES=${HOME}/nmt_data/toy_reverse/train/sources.txt
#export TRAIN_TARGETS=${HOME}/nmt_data/toy_reverse/train/targets.txt
#export DEV_SOURCES=${HOME}/nmt_data/toy_reverse/dev/sources.txt
#export DEV_TARGETS=${HOME}/nmt_data/toy_reverse/dev/targets.txt

export DEV_TARGETS_REF=${HOME}/nmt_data/toy_reverse/dev/targets.txt
export TRAIN_STEPS=1000





#export MODEL_DIR=${TMPDIR:-/tmp}/nmt_tutorial
export MODEL_DIR=${TMPDIR:-/tmp}/nmt_jessewho2
mkdir -p $MODEL_DIR


# enter into the python Venv which has our python libs/tensorflow etc.
workon dfl




nohup python -m bin.train \
  --config_paths="
      ./example_configs/nmt_small.yml,
      ./example_configs/train_seq2seq.yml,
      ./example_configs/text_metrics_bpe.yml" \
  --model_params "
      vocab_source: $VOCAB_SOURCE
      vocab_target: $VOCAB_TARGET" \
  --input_pipeline_train "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $TRAIN_SOURCES
      target_files:
        - $TRAIN_TARGETS" \
  --input_pipeline_dev "
    class: ParallelTextInputPipeline
    params:
       source_files:
        - $DEV_SOURCES
       target_files:
        - $DEV_TARGETS" \
  --batch_size 32 \
  --train_steps $TRAIN_STEPS \
  --output_dir $MODEL_DIR &





export PRED_DIR=${MODEL_DIR}/pred
mkdir -p ${PRED_DIR}

python -m bin.infer \
  --tasks "
    - class: DecodeText" \
  --model_dir $MODEL_DIR \
  --input_pipeline "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $DEV_SOURCES" \
  >  ${PRED_DIR}/predictions.txt

more ${PRED_DIR}/predictions.txt














head -111 ~//nmt_data/toy_reverse/train/202010292130vocab.sources.txt
head -111 ~//nmt_data/toy_reverse/train/202010292130vocab.targets.txt

head -1117 ~//nmt_data/toy_reverse/train/202010292130vocab.sources.txt > ~//nmt_data/toy_reverse/train/vocab.sources.txt 
head -1117 ~//nmt_data/toy_reverse/train/202010292130vocab.targets.txt > ~//nmt_data/toy_reverse/train/vocab.targets.txt 

cp who.txt  ~/nmt_data/toy_reverse/train/sources.txt
cp who.txt  ~/nmt_data/toy_reverse/train/targets.txt


head -1117 ~//nmt_data/toy_reverse/train/202010292130vocab.sources.txt > ~//nmt_data/toy_reverse/train/vocab.targets.txt 


cp ~/nmt_data/toy_reverse/train/sources.txt ~//nmt_data/toy_reverse/dev/sources.txt


export TRAIN_STEPS=199999
export TRAIN_STEPS=9999


export TRAIN_STEPS=999999







python -m bin.infer   --tasks "
 1104      - class: DecodeText"   --model_dir $MODEL_DIR   --input_pipeline "
 1105      class: ParallelTextInputPipeline
 1106      params:
 1107        source_files:
 1108          - $DEV_SOURCES"   >  ${PRED_DIR}/predictions.txt
 1109  more \$\{MODEL_DIR\}/pred/predictions.txt 
 1110  more $\{MODEL_DIR\}/pred/predictions.txt 
 1111  more ${MODEL_DIR}/pred/predictions.txt 

